---
title: "Project 2: Estimation of Fat in Fast Food Burgers"
author: "Seth Kimpler"
date: '`r format(Sys.time(), "%A, %B %d, %Y @ %I:%M %p")`'
output: 
  html_document: 
    theme: yeti
    highlight: textmate
---

```{r globalopts, include = FALSE}
knitr::opts_chunk$set(message = FALSE, warning = FALSE, comment = NA)
```

### **Packages/Data**

Load the packages and dataset here. Use `glimpse` to visualize the structure of the dataset.

Packages Used

```{r}
library("dplyr")
library("ggplot2")
library("infer")
library("moments")
library("tidyverse")
```

Fast Food Data

```{r}
FastFood2017 <- read.csv(file = url("https://raw.githubusercontent.com/STAT-JET-ASU/Datasets/master/Instructor/fastfood2017.csv"))
```

***
### **Problem**

Nutritionists recommend against eating fast food because it is high in sodium, saturated fat, trans fat, and cholesterol. Eating too much over a long period of time can lead to health problems such as high blood pressure, heart disease, and obesity. Many fast-food meals contain more than an entire day's worth of recommended calories! According to the [Cleveland Clinic](https://my.clevelandclinic.org/health/articles/11208-fat-what-you-need-to-know), a person eating the general 2000 calorie diet should have at most 22g of saturated fat per day. Use the [`fastfood2017.csv`](https://raw.githubusercontent.com/STAT-JET-ASU/Datasets/master/Instructor/fastfood2017.csv) dataset to perform the following analysis. The data are also available in the `resampledata` package.

#### **Part A**

Reduce the dataset so it contains only burgers. Create a new variable that indicates whether each burger has more than 22g of fat (or not). Compute summaries that show the sample size and the sample proportion of fast food burgers that have more than 22g of saturated fat.

```{r}
FastFood2017 <- FastFood2017 %>% filter(type == "Burger") %>% mutate(over_22g_satfat = ifelse(saturatedfat > 22, TRUE, FALSE))
FastFood2017 %>% summarize(Num_Total_Entries = n(), Prop_Over_22g_Satfat = sum(over_22g_satfat == TRUE) / n())
```

#### **Part B**

Use your sample of fast-food burgers to create a 95% bootstrap percentile interval to estimate the true proportion of all fast food burgers that have more than 22g of saturated fat.

```{r}
FF2017Boot <- FastFood2017 %>% specify(response = over_22g_satfat, success = "TRUE") %>% generate(reps = 1000, type = "bootstrap") %>% calculate(stat = "prop")
get_confidence_interval(FF2017Boot, level = 0.95)

```

#### **Part C**

Use your sample of fast-food burgers to create a 95% SE interval from your bootstrap distribution to estimate the true proportion of all fast food burgers that have more than 22g of saturated fat.

```{r}
# TODO 
```

#### **Part D**

Assess your bootstrap distribution for approximate normality using a density plot, boxplot, normal QQ plot, skewness calculation, and kurtosis calculation. 

```{r}
ggplot(FF2017Boot, aes(x = stat)) + geom_density() + labs(title = "Density Graph of Bootstrap Results", x = "Saturated Fat", y = "Density")
ggplot(FF2017Boot, aes(y = stat)) + geom_boxplot() + labs(title = "Boxplot of Bootstrap Results", y = "Density")
FF2017Boot %>% summarize(skewness = skewness(stat), kurtosis = kurtosis(stat))
```

#### **Questions**

In order for the 95% SE interval to be valid, the sampling distribution of $\hat{p}$ must be approximately normal. Does that requirement appear to be met here? Explain using the results of Part D.

**ANSWER:** That requirement does appear to be met. The graphs of `FF2017Boot` are well-balanced in nature with a natural curve. This is reflected in the skewness and kurtosis of the dataset. 

According to the description, the burgers selected for inclusion in the dataset for each restaurant were the smallest hamburger, the smallest cheeseburger, and some of the restaurantâ€™s most well known bigger burgers (e.g., Big Mac). The sample also does not include some smaller regional restaurants. Thus, it is not a purely random sample from the population. How might this affect your bootstrap intervals? Explain.

**ANSWER:** These calculations would not be as applicable to geographic areas where these large chains are not as prevelant, or where there is a larger proportion of the small chains. While it is not possible to say if factoring these smaller chains and other geographic areas in would shift the data in any direction, the lack of randomness in the provided data means that it cannot be considered truly universal. 


***
### Session Info

**Names of Collaborators**:

```{r}
sessionInfo()
```
